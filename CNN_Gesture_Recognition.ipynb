{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scipy==1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "#batch_size = #experiment with the batch size\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to crop image :\n",
    "\n",
    "def crop_img(img, scale=1.0):\n",
    "    center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n",
    "    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n",
    "    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "    img_cropped = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n",
    "    return img_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  7  8  9 11 15 16 19 20 21 22 26 27 28]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(20)\n",
    "random_numbers = np.random.randint(0, 31, size=20)  # 31 is exclusive\n",
    "arr = np.unique(np.sort(random_numbers))\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3  4  5  7  8 12 13 15 18 20 22 23 24 25 26 28 29 30]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(25)\n",
    "random_numbers1 = np.random.randint(0, 31, size=25)  # 31 is exclusive\n",
    "arr1 = np.unique(np.sort(random_numbers1))\n",
    "\n",
    "print(arr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = arr\n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,20,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.10)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        data_remaining = len(t)%batch_size\n",
    "        if(data_remaining != 0):\n",
    "            batch_data = np.zeros((data_remaining,20,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((data_remaining,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(data_remaining): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.10)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 5\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/datasets/Project_data/train'\n",
    "val_path = '/datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 5\n",
    "# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D,Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Base Model : Conv 3D Model with 5 epochs, 25 batch size\n",
    "Without dropouts in Conv layer and with batch normalization\n",
    "Input image size 100X100 , adam optimiser with learning rate 0.0002 with decay, 20 images as input out of 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConv3D_1 = Sequential()\n",
    "modelConv3D_1.add(Conv3D(8, (3,3,3), padding=\"same\", input_shape=(20, 100, 100, 3)))\n",
    "modelConv3D_1.add(Activation(\"relu\"))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_1.add(Conv3D(16, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_1.add(Activation(\"relu\"))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_1.add(Conv3D(32, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_1.add(Activation(\"relu\"))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_1.add(Conv3D(64, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_1.add(Activation(\"relu\"))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_1.add(Flatten())\n",
    "modelConv3D_1.add(Dense(64, activation='relu'))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_1.add(Dense(64, activation='relu'))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_1.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = tf.optimizers.Adam(lr=0.0002, decay=1e-6)\n",
    "\n",
    "# compile model :\n",
    "modelConv3D_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary :\n",
    "print (modelConv3D_1.summary())\n",
    "print(\"Total Params in model:\", modelConv3D_1.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#optimiser = #write your optimizer\n",
    "modelConv3D_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (modelConv3D_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1History = modelConv3D_1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result of Model 1 :\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(model1History.history['categorical_accuracy'])\n",
    "plt.plot(model1History.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model1History.history['loss'])\n",
    "plt.plot(model1History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Base Model : Conv 3D Model with 20 epochs, 30 batch size Without dropouts in Conv layer and with batch normalization Input image size 100X100 , adam optimiser with learning rate 0.0002 without decay, 20 images as input out of 30. Dropout changed to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConv3D_2 = Sequential()\n",
    "modelConv3D_2.add(Conv3D(8, (3,3,3), padding=\"same\", input_shape=(20, 100, 100, 3)))\n",
    "modelConv3D_2.add(Activation(\"relu\"))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_2.add(Conv3D(16, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_2.add(Activation(\"relu\"))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_2.add(Conv3D(32, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_2.add(Activation(\"relu\"))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_2.add(Conv3D(64, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_2.add(Activation(\"relu\"))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_2.add(Flatten())\n",
    "modelConv3D_2.add(Dense(64, activation='relu'))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(Dropout(0.5))\n",
    "\n",
    "modelConv3D_2.add(Dense(64, activation='relu'))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_2.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = tf.optimizers.Adam(lr=0.0002)\n",
    "\n",
    "# compile model :\n",
    "modelConv3D_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary :\n",
    "print (modelConv3D_2.summary())\n",
    "print(\"Total Params in model:\", modelConv3D_2.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "num_epochs = 20\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose =1 )\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2History = modelConv3D_2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result of Model 1 :\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(model2History.history['categorical_accuracy'])\n",
    "plt.plot(model2History.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model2History.history['loss'])\n",
    "plt.plot(model2History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third Base Model : Conv 3D Model with 20 epochs, 30 batch size Without dropouts in Conv layer and with batch normalization Input image size 100X100 , adam optimiser with learning rate 0.0002 without decay, 20 images as input out of 30. Dropout changed to 0.25. Image Scale Ratio: 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_updated(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = arr\n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,20,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.03)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        data_remaining = len(t)%batch_size\n",
    "        if(data_remaining != 0):\n",
    "            batch_data = np.zeros((data_remaining,20,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((data_remaining,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(data_remaining): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.03)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 20, 100, 100, 8)   656       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20, 100, 100, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 20, 100, 100, 8)  32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 10, 50, 50, 8)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 10, 50, 50, 16)    3472      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 10, 50, 50, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10, 50, 50, 16)   64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 5, 25, 25, 16)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 5, 25, 25, 32)     13856     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 5, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 5, 25, 25, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 12, 12, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 12, 12, 64)     55360     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 12, 12, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 12, 12, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 6, 6, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                147520    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 226,341\n",
      "Trainable params: 225,845\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "Total Params in model: 226341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:44:01.033938: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-11-05 13:44:01.034027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:3f:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "modelConv3D_3 = Sequential()\n",
    "modelConv3D_3.add(Conv3D(8, (3,3,3), padding=\"same\", input_shape=(20, 100, 100, 3)))\n",
    "modelConv3D_3.add(Activation(\"relu\"))\n",
    "modelConv3D_3.add(BatchNormalization())\n",
    "modelConv3D_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_3.add(Conv3D(16, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_3.add(Activation(\"relu\"))\n",
    "modelConv3D_3.add(BatchNormalization())\n",
    "modelConv3D_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_3.add(Conv3D(32, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_3.add(Activation(\"relu\"))\n",
    "modelConv3D_3.add(BatchNormalization())\n",
    "modelConv3D_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_3.add(Conv3D(64, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_3.add(Activation(\"relu\"))\n",
    "modelConv3D_3.add(BatchNormalization())\n",
    "modelConv3D_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_3.add(Flatten())\n",
    "modelConv3D_3.add(Dense(64, activation='relu'))\n",
    "modelConv3D_3.add(BatchNormalization())\n",
    "modelConv3D_3.add(Dropout(0.5))\n",
    "\n",
    "modelConv3D_3.add(Dense(64, activation='relu'))\n",
    "modelConv3D_3.add(BatchNormalization())\n",
    "modelConv3D_3.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_3.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = tf.optimizers.Adam(lr=0.0002)\n",
    "\n",
    "# compile model :\n",
    "modelConv3D_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary :\n",
    "print (modelConv3D_3.summary())\n",
    "print(\"Total Params in model:\", modelConv3D_3.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "num_epochs = 20\n",
    "train_generator_updated = generator_updated(train_path, train_doc, batch_size)\n",
    "val_generator_updated = generator_updated(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose =1 )\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267/3051150666.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model3History = modelConv3D_3.fit_generator(train_generator_updated, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/tmp/ipykernel_267/3659705889.py:15: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 30\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 13:44:22.395408: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/23 [===========================>..] - ETA: 2s - loss: 2.2425 - categorical_accuracy: 0.2288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267/3659705889.py:42: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 2.2409 - categorical_accuracy: 0.2278Source path =  /datasets/Project_data/val ; batch size = 30\n",
      "\n",
      "Epoch 00001: saving model to model_init_2024-11-0513_43_45.039785/model-00001-2.24095-0.22775-1.61294-0.23000.h5\n",
      "23/23 [==============================] - 84s 3s/step - loss: 2.2409 - categorical_accuracy: 0.2278 - val_loss: 1.6129 - val_categorical_accuracy: 0.2300 - lr: 2.0000e-04\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9796 - categorical_accuracy: 0.2534\n",
      "Epoch 00002: saving model to model_init_2024-11-0513_43_45.039785/model-00002-1.97957-0.25339-1.65554-0.23000.h5\n",
      "23/23 [==============================] - 76s 3s/step - loss: 1.9796 - categorical_accuracy: 0.2534 - val_loss: 1.6555 - val_categorical_accuracy: 0.2300 - lr: 2.0000e-04\n",
      "Epoch 3/20\n",
      "22/23 [===========================>..] - ETA: 2s - loss: 1.8919 - categorical_accuracy: 0.2864\n",
      "Epoch 00003: saving model to model_init_2024-11-0513_43_45.039785/model-00003-1.88593-0.28808-1.66655-0.22000.h5\n",
      "23/23 [==============================] - 54s 2s/step - loss: 1.8859 - categorical_accuracy: 0.2881 - val_loss: 1.6665 - val_categorical_accuracy: 0.2200 - lr: 2.0000e-04\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.8543 - categorical_accuracy: 0.2926\n",
      "Epoch 00004: saving model to model_init_2024-11-0513_43_45.039785/model-00004-1.85426-0.29261-1.67381-0.13000.h5\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.8543 - categorical_accuracy: 0.2926 - val_loss: 1.6738 - val_categorical_accuracy: 0.1300 - lr: 2.0000e-04\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7983 - categorical_accuracy: 0.2956\n",
      "Epoch 00005: saving model to model_init_2024-11-0513_43_45.039785/model-00005-1.79825-0.29563-1.67798-0.17000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.7983 - categorical_accuracy: 0.2956 - val_loss: 1.6780 - val_categorical_accuracy: 0.1700 - lr: 2.0000e-04\n",
      "Epoch 6/20\n",
      "22/23 [===========================>..] - ETA: 1s - loss: 1.8058 - categorical_accuracy: 0.3136\n",
      "Epoch 00006: saving model to model_init_2024-11-0513_43_45.039785/model-00006-1.80281-0.31373-1.73357-0.15000.h5\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.8028 - categorical_accuracy: 0.3137 - val_loss: 1.7336 - val_categorical_accuracy: 0.1500 - lr: 4.0000e-05\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7010 - categorical_accuracy: 0.3394\n",
      "Epoch 00007: saving model to model_init_2024-11-0513_43_45.039785/model-00007-1.70104-0.33937-1.64944-0.17000.h5\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.7010 - categorical_accuracy: 0.3394 - val_loss: 1.6494 - val_categorical_accuracy: 0.1700 - lr: 4.0000e-05\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7934 - categorical_accuracy: 0.3228\n",
      "Epoch 00008: saving model to model_init_2024-11-0513_43_45.039785/model-00008-1.79340-0.32278-1.67497-0.19000.h5\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.7934 - categorical_accuracy: 0.3228 - val_loss: 1.6750 - val_categorical_accuracy: 0.1900 - lr: 4.0000e-05\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7576 - categorical_accuracy: 0.3379\n",
      "Epoch 00009: saving model to model_init_2024-11-0513_43_45.039785/model-00009-1.75758-0.33786-1.68666-0.21000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.7576 - categorical_accuracy: 0.3379 - val_loss: 1.6867 - val_categorical_accuracy: 0.2100 - lr: 4.0000e-05\n",
      "Epoch 10/20\n",
      "22/23 [===========================>..] - ETA: 1s - loss: 1.6997 - categorical_accuracy: 0.3470\n",
      "Epoch 00010: saving model to model_init_2024-11-0513_43_45.039785/model-00010-1.70026-0.34691-1.73924-0.20000.h5\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.7003 - categorical_accuracy: 0.3469 - val_loss: 1.7392 - val_categorical_accuracy: 0.2000 - lr: 8.0000e-06\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7363 - categorical_accuracy: 0.3348\n",
      "Epoch 00011: saving model to model_init_2024-11-0513_43_45.039785/model-00011-1.73630-0.33484-1.66768-0.27000.h5\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.7363 - categorical_accuracy: 0.3348 - val_loss: 1.6677 - val_categorical_accuracy: 0.2700 - lr: 8.0000e-06\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7481 - categorical_accuracy: 0.3424\n",
      "Epoch 00012: saving model to model_init_2024-11-0513_43_45.039785/model-00012-1.74807-0.34238-1.67112-0.29000.h5\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.7481 - categorical_accuracy: 0.3424 - val_loss: 1.6711 - val_categorical_accuracy: 0.2900 - lr: 8.0000e-06\n",
      "Epoch 13/20\n",
      "22/23 [===========================>..] - ETA: 1s - loss: 1.6743 - categorical_accuracy: 0.3606\n",
      "Epoch 00013: saving model to model_init_2024-11-0513_43_45.039785/model-00013-1.67410-0.36199-1.71296-0.28000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5999999959603884e-06.\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.6741 - categorical_accuracy: 0.3620 - val_loss: 1.7130 - val_categorical_accuracy: 0.2800 - lr: 8.0000e-06\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7413 - categorical_accuracy: 0.3348\n",
      "Epoch 00014: saving model to model_init_2024-11-0513_43_45.039785/model-00014-1.74129-0.33484-1.82630-0.18000.h5\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.7413 - categorical_accuracy: 0.3348 - val_loss: 1.8263 - val_categorical_accuracy: 0.1800 - lr: 1.6000e-06\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7669 - categorical_accuracy: 0.3348\n",
      "Epoch 00015: saving model to model_init_2024-11-0513_43_45.039785/model-00015-1.76694-0.33484-1.86329-0.21000.h5\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.7669 - categorical_accuracy: 0.3348 - val_loss: 1.8633 - val_categorical_accuracy: 0.2100 - lr: 1.6000e-06\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6746 - categorical_accuracy: 0.3454\n",
      "Epoch 00016: saving model to model_init_2024-11-0513_43_45.039785/model-00016-1.67457-0.34540-1.74918-0.21000.h5\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.6746 - categorical_accuracy: 0.3454 - val_loss: 1.7492 - val_categorical_accuracy: 0.2100 - lr: 1.6000e-06\n",
      "Epoch 17/20\n",
      "22/23 [===========================>..] - ETA: 1s - loss: 1.7668 - categorical_accuracy: 0.2985\n",
      "Epoch 00017: saving model to model_init_2024-11-0513_43_45.039785/model-00017-1.76888-0.29864-1.75355-0.26000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000037395512e-07.\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.7689 - categorical_accuracy: 0.2986 - val_loss: 1.7535 - val_categorical_accuracy: 0.2600 - lr: 1.6000e-06\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7029 - categorical_accuracy: 0.3620\n",
      "Epoch 00018: saving model to model_init_2024-11-0513_43_45.039785/model-00018-1.70293-0.36199-1.71660-0.29000.h5\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.7029 - categorical_accuracy: 0.3620 - val_loss: 1.7166 - val_categorical_accuracy: 0.2900 - lr: 3.2000e-07\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6748 - categorical_accuracy: 0.3379\n",
      "Epoch 00019: saving model to model_init_2024-11-0513_43_45.039785/model-00019-1.67481-0.33786-1.70572-0.27000.h5\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.6748 - categorical_accuracy: 0.3379 - val_loss: 1.7057 - val_categorical_accuracy: 0.2700 - lr: 3.2000e-07\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6742 - categorical_accuracy: 0.3379\n",
      "Epoch 00020: saving model to model_init_2024-11-0513_43_45.039785/model-00020-1.67421-0.33786-1.74543-0.28000.h5\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.6742 - categorical_accuracy: 0.3379 - val_loss: 1.7454 - val_categorical_accuracy: 0.2800 - lr: 3.2000e-07\n"
     ]
    }
   ],
   "source": [
    "model3History = modelConv3D_3.fit_generator(train_generator_updated, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator_updated, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABA/0lEQVR4nO3dd3hU1fbw8e9KIwlpQEJJ6L2EHjooitIEQRSwICoq8rPhtVzLVW9577VcFbmgiChYEUEFG6CIdKmhSO811IQSkpCe/f5xJhghZZJMSTLr8zw8mZlzzpyVYXLWOWfvvbYYY1BKKeW5vNwdgFJKKffSRKCUUh5OE4FSSnk4TQRKKeXhNBEopZSH00SglFIeThOBUnYSkY9F5N92rntYRG4o7fso5QqaCJRSysNpIlBKKQ+niUBVKLZbMs+IyFYRSRGR6SJSQ0QWikiSiCwWkSp51r9ZRHaIyAURWSYiLfIsay8im2zbzQb8r9jXIBHZYtt2tYi0KWHMD4rIfhE5JyLfi0ik7XURkbdF5IyIXBSRbSISbVs2UER22mI7LiJPl+gDUwpNBKpiuhW4EWgKDAYWAi8AEVjf+ccBRKQpMAt4wrZsAfCDiPiJiB/wLfAZUBX4yva+2LZtD8wAHgKqAe8D34tIpeIEKiLXA68CI4BawBHgS9vivsA1tt8j1LbOWduy6cBDxphgIBpYUpz9KpWXJgJVEU02xpw2xhwHVgLrjDGbjTFpwDygvW29kcB8Y8wvxphM4E0gAOgOdAV8gYnGmExjzNfAhjz7GAu8b4xZZ4zJNsZ8AqTbtiuOu4AZxphNxph04Hmgm4jUBzKBYKA5IMaYXcaYk7btMoGWIhJijDlvjNlUzP0qdZkmAlURnc7zODWf50G2x5FYZ+AAGGNygGNAlG3ZcfPnqoxH8jyuBzxluy10QUQuAHVs2xXHlTEkY531RxljlgDvAO8CZ0RkmoiE2Fa9FRgIHBGR5SLSrZj7VeoyTQTKk53AOqAD1j15rIP5ceAkEGV7LVfdPI+PAf8xxoTl+RdojJlVyhgqY91qOg5gjJlkjOkItMS6RfSM7fUNxpghQHWsW1hzirlfpS7TRKA82RzgJhHpIyK+wFNYt3dWA2uALOBxEfEVkWFA5zzbfgCME5EutkbdyiJyk4gEFzOGWcB9ItLO1r7wCtatrMMi0sn2/r5ACpAG5NjaMO4SkVDbLa2LQE4pPgfl4TQRKI9ljNkDjAImAwlYDcuDjTEZxpgMYBhwL3AOqz1hbp5tY4EHsW7dnAf229YtbgyLgZeAb7CuQhoBt9sWh2AlnPNYt4/OAm/Ylt0NHBaRi8A4rLYGpUpEdGIapZTybHpFoJRSHk4TgVJKeThNBEop5eE0ESillIfzcXcAxRUeHm7q16/v7jCUUqpc2bhxY4IxJiK/ZeUuEdSvX5/Y2Fh3h6GUUuWKiBwpaJneGlJKKQ+niUAppTycJgKllPJw5a6NID+ZmZnExcWRlpbm7lCczt/fn9q1a+Pr6+vuUJRSFUSFSARxcXEEBwdTv359/lwssmIxxnD27Fni4uJo0KCBu8NRSlUQFeLWUFpaGtWqVavQSQBARKhWrZpHXPkopVynQiQCoMIngVye8nsqpVynwiSCoqRlZnMyMZXsHK22qpRSeXlMIsjIyiE+KZ20zGyHv/eFCxeYMmVKsbcbOHAgFy5ccHg8SilVHB6TCAL8vAFIdWEiyMrKKnS7BQsWEBYW5vB4lFKqOCpEryF7+HgJPl5epGY4PhE899xzHDhwgHbt2uHr64u/vz9VqlRh9+7d7N27l6FDh3Ls2DHS0tIYP348Y8eOBf4ol5GcnMyAAQPo2bMnq1evJioqiu+++46AgACHx6qUUleqcIngnz/sYOeJi/kuS8vMxgABvt7Fes+WkSH8fXCrApe/9tprbN++nS1btrBs2TJuuukmtm/ffrmL54wZM6hatSqpqal06tSJW2+9lWrVqv3pPfbt28esWbP44IMPGDFiBN988w2jRo0qVpxKKVUSFS4RFMbLS8jMcv4c3507d/5TP/9JkyYxb948AI4dO8a+ffuuSgQNGjSgXbt2AHTs2JHDhw87PU6llIIKmAgKO3NPvJTBkXOXaFw9iEA/5/3qlStXvvx42bJlLF68mDVr1hAYGEjv3r3zHQdQqVKly4+9vb1JTU11WnxKKZWXxzQWQ54GYwe3EwQHB5OUlJTvssTERKpUqUJgYCC7d+9m7dq1Dt23UkqVVoW7IiiMr7cX3l7i8J5D1apVo0ePHkRHRxMQEECNGjUuL+vfvz9Tp06lRYsWNGvWjK5duzp030opVVpiTPkaYBUTE2OunJhm165dtGjRwq7tD8Ynk2MMjasHOyM8lyjO76uUUgAistEYE5PfMo+6NQRWj6HUzBxyylkCVEopZ3FaIhCROiKyVER2isgOERmfzzp3ichWEdkmIqtFpK2z4skV4OeNMYb0TOf3HlJKqfLAmW0EWcBTxphNIhIMbBSRX4wxO/Oscwi41hhzXkQGANOALk6MCX/fP0YY5zYeK6WUJ3NaIjDGnARO2h4nicguIArYmWed1Xk2WQvUdlY8uSr5eOEl4pSaQ0opVR65pI1AROoD7YF1hax2P7CwgO3HikisiMTGx8eXNharncAJpSaUUqo8cnoiEJEg4BvgCWNMvrUfROQ6rETwbH7LjTHTjDExxpiYiIiIUsfk7+dNamY25a3HlFJKOYNTE4GI+GIlgZnGmLkFrNMG+BAYYow568x4cgX4epNjDOkOKjdR0jLUABMnTuTSpUsOiUMppUrCmb2GBJgO7DLGTChgnbrAXOBuY8xeZ8Vypdyic45qJ9BEoJQqz5zZa6gHcDewTUS22F57AagLYIyZCrwMVAOm2KZgzCpowIMjVfL1QsQaYRzmgPfLW4b6xhtvpHr16syZM4f09HRuueUW/vnPf5KSksKIESOIi4sjOzubl156idOnT3PixAmuu+46wsPDWbp0qQOiUUqp4nFmr6FVQKET7BpjHgAecOiOFz4Hp7YVuooX0CgzC0HAnpLUNVvDgNcKXJy3DPWiRYv4+uuvWb9+PcYYbr75ZlasWEF8fDyRkZHMnz8fsGoQhYaGMmHCBJYuXUp4eHhxfkullHIYjxtZnMtbhBxjMDi2wXjRokUsWrSI9u3b06FDB3bv3s2+ffto3bo1v/zyC88++ywrV64kNDTUoftVSqmSqnhF5wo5c88rOTmd4xdSaV4zGD8fxw0sM8bw/PPP89BDD121bNOmTSxYsIAXX3yRPn368PLLLztsv0opVVIee0XgyDmM85ah7tevHzNmzCA5ORmA48ePc+bMGU6cOEFgYCCjRo3imWeeYdOmTVdtq5RS7lDxrgjs5O/jjSCkZuQQWsqpgfOWoR4wYAB33nkn3bp1AyAoKIjPP/+c/fv388wzz+Dl5YWvry/vvfceAGPHjqV///5ERkZqY7FSyi08rgx1XntPJ+Hr7UWD8MpFr1yGaBlqpVRxaRnqAmipCaWU8vRE4OdNVk4Omdlaklop5bkqTCIoyS2u3BHG5emqoLzdylNKlX0VIhH4+/tz9uzZYh8k885NUB4YYzh79iz+/v7uDkUpVYFUiF5DtWvXJi4ujpKUqD53MY2LJ4VzQZWcEJnj+fv7U7u206dtUEp5kAqRCHx9fWnQoEGJtp0yazObjpzjt+eud3BUSilVPlSIW0OlER0ZwvELqZxPyXB3KEop5RaaCKKsmj87TuQ7Z45SSlV4Hp8IWkWGALD9RKKbI1FKKffw+EQQFuhHVFgA249rIlBKeSaPTwQA0VEhemtIKeWxNBEA0ZGhHEpIISkt092hKKWUy2ki4I8G4516VaCU8kCaCIBWUbkNxpoIlFKeRxMBUD3Yn+rBldihPYeUUh5IE4FNq8gQdhzXKwKllOfRRGATHRXKvjNJ5aoSqVJKOYLTEoGI1BGRpSKyU0R2iMj4fNZpLiJrRCRdRJ52Viz2aBUZSo6B3af0qkAp5VmceUWQBTxljGkJdAUeEZGWV6xzDngceNOJcdglWhuMlVIeymmJwBhz0hizyfY4CdgFRF2xzhljzAbA7R34o8ICCAv0ZYeOMFZKeRiXtBGISH2gPbDOFfsrCREhOjJURxgrpTyO0xOBiAQB3wBPGGNKdJQVkbEiEisisSWZfMZerSJD2HMqiYwsncNYKeU5nJoIRMQXKwnMNMbMLen7GGOmGWNijDExERERjgvwCq2iQsnIzmHfmSSn7UMppcoaZ/YaEmA6sMsYM8FZ+3GkaFtJah1PoJTyJM6cqrIHcDewTUS22F57AagLYIyZKiI1gVggBMgRkSeAliW9hVRa9atVprKfN9tPJDKCOu4IQSmlXM5picAYswqQItY5BZSZmdi9vIRWkaE6N4FSyqPoyOIrtIoKYdfJJLJzjLtDUUopl9BEcIVWkaGkZmZzKCHZ3aEopZRLaCK4wuURxtpgrJTyEJoIrtA4IohKPl7aTqCU8hiaCK7g4+1F81ohbNe5CZRSHkITQT6ibXMT5GiDsVLKA2giyEd0VChJ6VkcO3/J3aEopZTTaSLIR6vcEcZagE4p5QE0EeSjaY1gfLxEG4yVUh5BE0E+/H29aVIjWCepUUp5BE0EBbAajBMxRhuMlVIVmyaCAkRHhXI2JYNTF9PcHYpSSjmVJoIC6AhjpZSn0ERQgBa1QhBBG4yVUhWeJoICBPr50DC8snYhVUpVeJoIChEdFcoOLTWhlKrgNBEUIjoylJOJaSQkp7s7FKWUchpNBIVoFaUjjJVSFZ8mgkK0igwFtMFYKVWxaSIoRGiAL3WrBmo7gVKqQtNEUIRWkSF6a0gpVaFpIihCdFQoR85eIjE1092hKKWUU2giKEJuSeqdelWglKqgnJYIRKSOiCwVkZ0iskNExuezjojIJBHZLyJbRaSDs+IpqdwGY20nUEpVVD5OfO8s4CljzCYRCQY2isgvxpidedYZADSx/esCvGf7WWZEBFeiZoi/9hxSSlVYTrsiMMacNMZssj1OAnYBUVesNgT41FjWAmEiUstZMZVUdFSIzk2glKqwXNJGICL1gfbAuisWRQHH8jyP4+pkgYiMFZFYEYmNj493WpwFaRkZyoH4ZC5lZLl830op5WxOTwQiEgR8AzxhjCnRabUxZpoxJsYYExMREeHYAO0QHRmCMbDrZJLL962UUs7m1EQgIr5YSWCmMWZuPqscB+rkeV7b9lqZEh2lDcZKqYrLmb2GBJgO7DLGTChgte+B0bbeQ12BRGPMSWfFVFK1Qv2pWtlPG4yVUhWSM3sN9QDuBraJyBbbay8AdQGMMVOBBcBAYD9wCbjPifGUmIjQKjJEZytTSlVITksExphVgBSxjgEecVYMjhQdFcoHKw6SnpVNJR9vd4ejlFIOoyOL7dQmKpSsHMOqfQnuDkUppRxKE4Gd+rSoQd2qgbzx8x6yc4y7w1FKKYfRRGAnPx8vnu7XjN2nkpi3ucx1bFJKqRLTRFAMg1rXonVUKBMW7SEtM9vd4SillENoIigGLy/h+QHNOZGYxierD7s7HKWUcghNBMXUvXE4vZtF8O7S/Vy4lOHucJRSqtQ0EZTAs/2bk5SexZRlB9wdilJKlZomghJoUSuEYe1r8/Hqwxy/kOrucJRSqlQ0EZTQk32bAvDWoj1ujkQppUpHE0EJRYUFcF/3+szbfFynsVRKlWuaCErh4d6NCfH35bWfdrs7FKWUKjG7EoGIjBeREFuV0OkisklE+jo7uLIuNNCXR65rxIq98fy2X0tPKKXKJ3uvCMbYJpXpC1TBqir6mtOiKkdGd6tPVFgAry7cRY6WnlBKlUP2JoLcKqIDgc+MMTsoorKop/D39eapvk3ZfvwiP2w94e5wlFKq2OxNBBtFZBFWIvhZRIKBHOeFVb4MbRdFi1ohvLloD+lZWnpCKVW+2JsI7geeAzoZYy4BvpTRSWTcwctLeG5Ac46dS2Xm2qPuDkcppYrF3kTQDdhjjLkgIqOAFwGdtzGPa5qE06NxNSYv2cfFtEx3h6OUUnazNxG8B1wSkbbAU8AB4FOnRVUOiQjP9W/B+UuZvL9cS08opcoPexNBlm1aySHAO8aYd4Fg54VVPrWuHcrNbSOZvuoQpxLT3B2OUkrZxd5EkCQiz2N1G50vIl5Y7QTqCs/0a0Z2jmHi4r3uDkUppexibyIYCaRjjSc4BdQG3nBaVOVYnaqBjOpajzmxx9h3Osnd4SilVJHsSgS2g/9MIFREBgFpxhhtIyjAY9c3obKfD69r6QmlVDlgb4mJEcB6YDgwAlgnIrcVsc0METkjItsLWF5FROaJyFYRWS8i0cUNvqyqWtmPcb0bsXjXGdYfOufucJRSqlD23hr6G9YYgnuMMaOBzsBLRWzzMdC/kOUvAFuMMW2A0cD/7IylXBjTowE1Q/x5deEurHZ2pZQqm+xNBF7GmDN5np8taltjzAqgsNPhlsAS27q7gfoiUsPOeMq8AD9v/nJjEzYfvcBP20+5OxyllCqQvYngJxH5WUTuFZF7gfnAglLu+3dgGICIdAbqYTVCVxi3dqhNk+pB/PfnPWRma0UOpVTZZG9j8TPANKCN7d80Y8yzpdz3a0CYiGwBHgM2A/kW6hGRsSISKyKx8fHxpdyt6/h4e/Fs/+YcSkjhyw3H3B2OUkrlS5x5/1pE6gM/GmMKbQgWEQEOAW1s5a4LFBMTY2JjYx0XpJMZYxj5/loOJqSw/JneVK7k4+6QlFIeSEQ2GmNi8ltW6BWBiCSJyMV8/iWJSKnmZxSRMBHxsz19AFhRVBIoj0SE5wY2JyE5nQ9WHnR3OEopdZVCT0+NMSUuIyEis4DeQLiIxAF/xzYa2RgzFWgBfCIiBtiBVeG0QupQtwoDW9dk2oqDdG1Yja4Nq7k7JKWUusxp9ymMMXcUsXwN0NRZ+y9rnh/Qgm3HE7l92lpu61ibFwa2oGplv6I3VEopJ9PJ612kTtVAFj1xLf/XuxHfbj5On7eW8VXsMR1joJRyO00ELhTg582z/Zsz//FeNIoI4pmvt3L7tLXsP5Ps7tCUUh5ME4EbNKsZzJyHuvHqsNbsOnmRAf9bwYRFe0jL1GkulVKup4nATby8hDs612XJ070Z1CaSSUv203/iClbtS3B3aEopD6OJwM3Cgyrx9sh2fH5/FwBGTV/HE19uJj4p3c2RKaU8hSaCMqJnk3B+euIaHu/ThAXbTtHnrWV8se4oOTnamKwqgEvn4Nd/wVmdxrUs0kRQhvj7evPkjU1ZML4XLWqF8MK8bQx/fw27T1W4cXbKkxz+Dab2hJVvwS8vuzsalQ9NBGVQ4+pBfDm2K28Ob8vB+GQGTVrFawt3k5qhjcmqHMnOgqWvwieDwKcStBoGu+fDOR1hX9ZoIiijRITbOtZmyVO9GdYhiqnLD9B34nIOxGtXU1UOXDgGnwyG5a9Bm5Hw0Aro9wp4+cDaqe6OTl1BE0EZV6WyH/+9rS2zx3YlNSOb0dPXcyoxzd1hKVWwnd/B1B5waivcMg1umQqVgiGkFrS+DTZ/Dqnn3R2lykMTQTnRpWE1Pr6vMxcuZXDPjPUkXsp0d0hK/VnGJfjhCZgzGqo2gnEroe3IP6/T7RHITIGNn7glRJU/TQTlSHRUKNNGx3AoIYUHPt2gA9BU2XF6J3xwPWz8CHqMhzE/Q9WGV69XszU0uBbWvQ/ZejJTVmgiKGd6NA7n7ZHtiD1ynke/2EyWznym3MkY2PAhfHAdXDoLo+bCjf8Cn0IKKnZ7FJJOwI5vXRamKpwmgnLopja1+NfNrVi86zQvzNumheuUe1w6B7NHwfynoH5P+L/V0LhP0ds1vgHCm8KayVYiUW6niaCcurtbfR6/vjFzYuN44+c97g5HeZrcsQF7f7Z6A935FQRF2Letlxd0fRhO/g5HfnNunMoumgjKsb/c2JQ7OtdlyrIDzFh1yN3hKE+QnQVLX7GNDfCHBxZbDcBexTyUtL0dAqvBmnedE6cqFp1AtxwTEf49NJrzKRn868edVAvyY0i7KHeHpSqqC0fhmwfh2FpodxcM+C9UCirZe/kGQKcHYPl/IWE/hDd2bKyqWPSKoJzz9hIm3t6OLg2q8vRXv7Nib7y7Q1IVUcpZmNYbTu+AYR/C0CklTwK5Oj0A3r6w7j2HhKhKThNBBeDv680H98TQuHow4z7fyO/HLrg7JFXRxE63egXdNx/aDHfMewZVhzYjYPNMq+FZuY0mggoixN+XT+7rRLUgP+77eIOWolCOk5kG66dBk75Qq61j37vrI5CVCrEzHPu+qlg0EVQg1UP8+XRMFwQYPX09py9qKQrlANvmQEq81f/f0Wq0hEbXW4kmS+fgcBdNBBVMg/DKl0tRjJ6+nsRUHb2pSsEYq2dPzdbQ4Brn7KPbI5B8GrbPdc77qyJpIqiAWtcO5f27YziYkMwDn2gpClUK+xdD/G7o9hiIOGcfjfpARAsr4egAM7dwWiIQkRkickZEthewPFREfhCR30Vkh4jc56xYPFHPJlqKQjnA6skQHAnRw5y3DxHrquD0Nji0wnn7UQVy5hXBx0D/QpY/Auw0xrQFegNviUghBUpUcQ1qE8k/BlulKP42b7tdpSiMMSReymT/mWTWHTzL/K0n+WT1Yd5atIf/zN+pt5o8ycmtcGg5dHnI6ubpTK2HQ+UIHWDmJk4bUGaMWSEi9QtbBQgWEQGCgHNAlrPi8VT3dK9PQnI6k5fsJzTQl95NI4hPTichOYOE5HQSktKtn7bnZ5MzyMjn6sFLIMeAr7cXf+3f3A2/iXK5Ne+CXxB0vNf5+/L1h04PwrJXIH4vRDR1/j7VZe4cWfwO8D1wAggGRhpj8r1/ISJjgbEAdevWdVmAFcWTNzYlITmDaSsOMm3FH9ME+ngJ1YL8CA+qRHhQJZrWCCY82I8I2/PwoEqEB1vLqwT68cTsLXyy+jAP9GpI1cp68VahXTwB27+2Ds4BYa7ZZ6f7rXmN174Lg//nmn0qwL2JoB+wBbgeaAT8IiIrjTFXzdRujJkGTAOIiYnR1qRiyi1FcVPrWnh7CRHBflSrXInQAF+8vOxvAHz8+sb8uPUEH6486PKrguT0LKYs3c/Q9lE0rRHs0n17pHXvg8mBruNct8/K4VYNot+/hOtfsp4rl3Bnr6H7gLnGsh84BOg9Byfx9hJ6NgmnW6NqNK4eTJXKfsVKAgBNagQzqE0kn6w+zLmUDCdFmr/3lu1nyrIDDJq8io9+O0ROjp4POE16sjXBTIuboUp91+672yOQlaYDzFzMnYngKNAHQERqAM2Ag4Vuodzu8esbcykzmw9Xuu6/6mRiKh+uPMSNLWvQs3E4//xhJ/d8pAPmnGbz55CWCN0fc/2+I5pZI5jXT7NGNCuXcGb30VnAGqCZiMSJyP0iMk5Ecq81/x/QXUS2Ab8CzxpjEpwVj3IMd1wVvLVoL8bAy4NaMv2eGP49NJoNh8/Rb+IKftp+0iUxeIycbFg7Bep0hdox7omh2yPWSObtX7tn/x7IaYnAGHOHMaaWMcbXGFPbGDPdGDPVGDPVtvyEMaavMaa1MSbaGPO5s2JRjuXKq4KdJy7yzaY47u1RnzpVAxERRnWtx/zHe1GnSiDjPt/EX7/+neR07XDmELt+gAtHrIOxuzS4FmpE6wAzF9KRxarYXHlV8OrCXYT4+/JI7z/Xq28UEcTch7vz6HWN+XpjHAP/t5KNR847NRaPsOYdqNIAmt/kvhhyB5id2QkHlrgvDg+iiUCViCuuClbsjWflvgQeu74xoYFXD2jy9fbi6X7NmP1QN3KMYfjU1Uz4ZS+ZOoq6ZI6ug7gN1jSSXt7ujSX6VgiqoQPMXEQTgSoRZ18VZOcYXlmwizpVA7i7W71C1+1UvyoLxvdiaPsoJv26j9umruFQQorDY6rw1kwG/zBof5e7IwGfStD5QTjwK5ze6e5oKjxNBKrEnHlV8M2mOHafSuKv/ZpTyafos9MQf18mjGjHO3e253BCCjdNWsmX64/aVVZDAecOwq4fIWYM+FV2+NsbY9hy7ALpWcUogBhzP/gEWI3XpZSSnsXt09bw9ca4Ur9XRaSJQJWYs64KUjOyeWvRHtrWCWNQm1rF2nZQm0h+eqIX7euG8dzcbYz9bCNnk7XOfZHWvgdePtB5rEPf1hjD4p2nGfzOKoa++xuvzN9l/8aBVaHdnbB1DiSfKVUc01cdYu3Bczw/dytbdAa/q2giUKXijKuC6asOcvpiOn8b2AIpQenjWqEBfDamCy/e1ILle+LpN3ElS/eU7kBSoV06Z40daD0cQoqXeAtijGHJ7tMMefc3Hvg0loupWXRvVI2Z645ysDiz53V9GLLTYcP0EseSkJzO+8sPcE3TCGqE+PPw5xtdPiCyrNNEoErF0VcFCcnpTF1+kBtb1qBzg6olfh8vL+GBXg357tEeVKvsx30fbeDl77YX79aEp9j4EWReckiXUWMMS/ecYeiU1Yz5OJZzKRn899Y2/PrUtfzv9vZU8vHivz/tsf8NwxtD0wGw4QPITC1RTO8s2U9aVg5/H9yS9+7qSEJyBuO/3Ey2jk6/TBOBKjVHXhX8b/E+UjOzeW6AY6qNtKgVwneP9uD+ng34dM0Rnpz9u5anyCsrA9ZNg4bXQc3oEr+NMYble+MZ9t5q7vtoAwlJ6bw2rDVLn+7NiE518PX2IiK4Eg9d24ifdpwi9nAxJqvv9ghcOgtbZxc7riNnU5i57ggjO0bRKDib1rVD+cfNrVi5L4FJv+4r9vtVVO4sOqcqiLxXBaWpTHogPpkv1h/ljs51aBQR5LD4/H29eWlQS2qG+POfBbuoEeLPy4NbOuz9y7XtX0PyKRhasm6axhhW7U9g4uJ9bDxynqiwAF65pTW3dayNn8/V55kP9GrA52uP8MqCXXzzf93tu/VXvyfUbANrpkD70eDlZQ00y0ixRiCnJNh+xl/13Of4MVb7nCV8exJsy4Em/bhjyLtsPFKbSUv20b5uGL2bVS/R716RaCJQDuGIyqSvL9yNv48X4/s4pxb9A70acCIxlRm/HSIyzJ8HejV0yn4czRhD3PlUdp68yLVNI/D3dVAff2Ng9TtQvaU1XWQxY1p94CwTF+9lw+Hz1Ar1599DoxkeU7vQXl6Bfj481bcpz36zjYXbTzGwtR1tEiJW3aO5D8LUnpCeZB3oswq4VeQXDJXDueRblR2pVahRqzURzZqAyYY1U5CpPXll8HvsOBHME7O38ONjPaldJbBYv39Fo4lAOURprwrWHzrHop2neerGpkQEV3JKjCLCSze15PTFNP49fxfVQ/y5uW2kU/ZVGlnZOew8eZHYw+eJPXKO2MPnOZNk9Xwa3rE2bwxv65gdHVwKZ3bAkHeLNR/xmgNneXvxXtYfOkeNkEr8a0grRnaqY1c3X4DbOtZhxqrDvP7Tbm5oUSPfK4ertLoFdv9oXQVUbmOVqK4cAYG2n5Xz/PQNAOCh6evY7pvI8jHXgb9tQGL0bfD1GCrNGsaXHR7huo3deWTmJuaM62Z3/G6Rkw3LXoVmAyCqo8PfXhOBcpiSXhUYYw0eqxFSyeln6V5ewoQR7UhIWs/Tc34nIqgS3RpVc+o+i3IxLZPNRy+w8fA5Nhw+z5ZjF0jNtBq1o8IC6NaoGjH1qrDvTDKfrjnCjS1r0LdVzdLvePU71ujd1sPtWj328DneXLSHtQfPUT24Ev8Y3JLbO9ct9hWKt5fw3MDm3PfRBmauO8J9PRrYsZEvjPjU7n2s3GeNSn9pUEtC/POMSq8ZDWOXwc/PE7bxHZZWXcHg4/fx/34M5d9DWxfr93CZlLPwzRg4uMx6rolAlWUlvSqYv+0kW45d4L+3tiHAz/lnZf6+3nwwOobbpq5m7GexfDWuG81rhhS+UXoyrH8f2o2C4Bol3rcxhuMXUtl45Dyxh8+z4fA59pxOwhhrOtCWkSGM7FSHjvWqEFO/CrVCAy5vm5GVQ+zh8zw/dxsd6lUhPKgUV06nd1qjdq9/0RrFW4QVe+O556P1hAdV4uVBLbmzS/ETQF69m0bQo3E1Jv26j2EdahMa4Lg5kXNyDK//tJuosABGdc1nRkO/QGsGtIbXEfbD4ywO+BvPrB/DvHpVuKV9bYfF4RBxG2HOaOtW2M2TocNop+xGytvIy5iYGBMbG+vuMFQB9p1Oou/EFYy7thHP2nFVkJ6VzY0TVhDo5838x3vhXczJckrj+IVUhk35DUGY+3B3IsMC8l8x4xLMHA5HVkH9XjD6u2LX4jl+IZW3ft7D6gNnOWWbRyGokg/t64ZZB/16VWlXN4ygSoWfm+05lcTgyau4rnkEU0d1LNE4CwC+fQS2fwNP7rQGbhXiVGIaAyetJDzIj3kP96ByETHaa/vxRAa/s4qHrmnksF5iAN9tOc74L7fw9si2RR/YLxzFfPMAcmwdc3OupdWD79OsjmPGUpSKMbDxY1j4VwiqCSM/hcj2pXpLEdlojMm3trh2H1X2ObMLvrzLKkVQiOKOK/h87VGOnrvEcwOauzQJgHXb5eP7OpOSnsW9H60nMTXz6pUy02D2XXDkN2gzEg6vhNWTi7WfH34/Qf+JK/hpxyk6NajKv4a0Yv7jPfn973357P4uPHFDU3o2CS8yCQA0qxnM0/2a8vOO08zddLxYcVyWdBq2zbFqChWRBLKyc3hs1ibSMrOZcldHhyUBgOioUG5pF8WM3w5x/ELJxghcKSMrhzcX7aFFrRCGtI0qeoOwusi9C0jp+hRDvFYSOON6Ug67+UQzMxW+exR+fMI68XhoeamTQFE0EaiiXTgKnw2zGut+fLLIGvGPX9+Y1MxsPihiXEFiaiaTl+yjZ+Nwrm0a4ciI7daiVgjv392RQwkpjP009s8DzrIy4Kt7rFLIQ96BW96HlkNgyb/hxOYi3zspLZMnZ2/hsVmbaVw9iIXjezH5jvaM7lafVpGhJU589/dsSOcGVfnH9ztKdgBdPw2yM61Ru0V4c5HVK+jVYa1pXN1xXXpzPdWvGQBv/VyMQWaF+GLdEY6dS+XZ/s3sn4rV24fK/V9mT9+Z+OSkUenjfpjVkyHHDVVszx+G6X1hy+dw7bNw11dFJmtH0ESgCpeSAJ/dApkp0Pkhq6fJzm8L3cTeq4IpS/eTmJrJ8wObl/wWhwN0bxzOm8Pbsu7QOZ6aYxtwlp1lNdDt/QlumgDtR1k9awZNtHqnfPOg1YOlALGHzzFw0kq+3XKcJ25owlcPdaNeNccUc/P2Et4a3pYcY3jmq2IOkMtIgdjp0GwgVGtU6Kq/7jrN1OUHuKtLXYa0s+PsugSiwgIY06MB87YcZ/vxxFK9V1JaJpOW7Kdbw2olOrFo2X0gi675ml+z2yGLXoQvhpe6xlGx7F0E719rTQx05xy47gWXlQPXRKAKlp4EM2+DxDjri9nvFajZGn56wWo8LURRVwVx5y/x0erDDGtfm1aRoc6IvliGtIvi+QHN+XHrSV6dvx3mPWTN1tXvVeh0/x8rBlaFYe/D2f3w89+uep/M7BwmLNrDiPfXIAhfjevOEzc0xcfbsX9qdaoG8vLglqw+cJZP1hy2f8MtX0Dqeej+aKGrxZ2/xJNzfqdVZAgvDXLu4LuHr2tEWIAvryzYVapqsR+sPMS5lAyeG1DyE4vR13dgXpPXeSlrDDmHVsJ7PWD/ryWOyS45ObD0VfhiBITWgbHLoWk/5+7zCpoIVP6y0q02gZNbrW57dbuCt491dpx0Apa/XujmRV0VvPnzHgR4up9zBo+VxNhrGnJft7o0Wfc3a8Rtn79Dt3xunzS4Bno8btXo2T3/8suHE1IYPnUNk5bsZ1iH2iwY34uO9ao4Ld4RMXXo07w6ry3czf4zSUVvkDsfcWQHqNutwNUysnJ45IvN5OQYptzVwXED2AoQ4u/L432asPrAWZbtiS/Re5xJSuPDlQe5qU0t2tYJK3EsIsJ/R7RlZejN3CmvkeVfBT4fBotesm4VOtqlc1YCWP4atL0D7l8EVe3oTutgmgjU1XKyrVGch5Zbg43ynp3U6WzdJlk7xWpALkRBVwXb4hL5dssJ7u/Z4E/dI91NgJe8ZjDCZzkTs4bxY+jtBa983YtQqy189yjm4knmbDjGwEkrOZSQwrt3duDN4W3tavwtVbwivHprawL9vHlyzu9Fz8y2Z6HV2N/90UIHkL26cBe/H7vAG8PbOOx2VlHu6lKP+tUCeXXhLrJKMMPc5F/3k5GVw9N9m5U6lhB/X94b1ZEt6bUY4/s6OR3vg9WTYEZf2Pk9XDxR6n0AcPJ3mHatNT5g0NswdIrVtdUNNBGoPzMGFjwNO7+Dvv+Bdndcvc4N/wS/IJj/dKENx/ldFeQOHqta2Y9xvQu/R+1SxsBPz+O1cQZZ3cbzW9QDPDn7d9YePJv/+j5+MOxDTGYqO98bxbPfbKFt7TB+eqIXNxVzDoXSqB7szyu3tGZrXCLvLt1f8IrGWPMRh9aFFkMKXG3htpN89NthxvRoQP9o1/0efj5e/LV/c/aeTi725DGHElKYtf4od3SuS4NwxySuFrVC+M/Q1qw4fIn/+oyDkZ9bDblz7oYJLeCtFjB7FKx6Gw6tLPJW6VU2f241Cudkw5ifrAmB3NhOpolA/dmyVyF2BvT8S8H3kSuHww1/t/rVb/uq0Le78qpg6Z4zrDl4lvF9mvx5xKc7GQO//hPWvQdd/g+fvv/kg3s6UbdaIGM/jWXv6fxvu6y6UJXXzWhapcYyM3oTMx/o4pYrnAGtazGsfRSTl+xna9yFq1fIyrC6Ih5dY93q8s7/SuVwQgp//Xor7eqEObRfv70GRNekQ90wJvyyl0sZWXZv9+bPe/Dz8eKxPo0dGs+tHWtzZ5e6TF1+gEU5neDJ3XD/Yuj/GtTvAae2w+J/wCeD4LU6MKW71e1z48dwapvV4eBKWenww3j47hHr6vqhFVA73679LuW0AWUiMgMYBJwxxlxV31ZEngFyJ0f1AVoAEcaYQuvT6oAyJ1o3DRY+Y936ufmdws9QcrLhwxushuTHYsG/4Abfx2Zt5tddp1n2TG/u+mAdWTmGRX+5Bl8HN6CW2LLXYdkr1lnZTRMu/95x5y8xbMpqvL2sAWe5B/n0rGze/HkPH6w8ROOIysyt+i4hccvgwSVWY7obJKZm0n/iHwPzLt/XT0mA2XfD0dVWcr/+pXx7oqRlZjNsympOJKYy//FeRBU0uM7JNh45x63vreEvNzRl/A1Nilx/y7ELDH33Nx7v04Qnb3R8e1NaZjbDp67h8NkUfni0J/WvvOJIOQvHN9r+xVo/U89by3wDrf7/UR2tf2F1Yf5TcGKT9X9x3YsFJmVnKGxAmTMTwTVAMvBpfonginUHA38xxlxf1PtqInCSbV/DNw9A85tg+Cf2fUGPb4IProcu42DAawWuljvauEn1IPaeTmbqqA4uve1QqFVvW2d17e6ykp/Xn5PTjhOJjHx/LbWrBDBnXDdOJaYx/sst7Dp5kdHd6vH8gBYEZF6A97pBQBWrjo2vew6iq/YlMGr6Osb0aGCV2T61HWbdASlnrN+tTcE1hV6Yt40v1h1lxr0xXN+85CU0HGHcZxtZsS+eZc/0pnqwf4HrGWO444O17DudzLJnehPspCvMY+cuMWjyKiLDApj3cPfCG8+Nsdphjm+EuFgrOZzaBtm2huZKITD0PWgxyCmxFsYtI4uNMSsAe2efuAOY5axYVBH2L7a6S9brAbdOt/8sJaqDdRa9/n3ry16A3LaCvaeTialXhX6OKJjmCGvfs5JA9K1WHRevq/8cWkWGMnVUR/afSWbE1DUMnryK+KQ0Ztwbw7+GRFu1kSpXs/6443fDLy+7/vew6dkknHu712fGb4fYvXSm7R50Jty3oNAk8O3m43yx7ijjrm3k9iQA8OyA5mRk5TBxceETxyzfG8/ag+d47PrGTksCYHXVnTiyHbtOXmTQ5FUs2X264G6uItb4jDYjYOB/ravE5+PggSUweJJ1K8gNSaAobr82F5FAoD/wjbtj8UhxsTB7NFRvAXd8Ab4Fn4Hlq89L1pnw/KcKHYn5xA1NaBRRmZcGtXTr4LHLYmfAT89Bi8HWiOFCBu70bBLOG8PbsPtUEj0ah7Nw/DVXHzAb94Guj1ijdvcucnLwBXu2XzNeDplP8+UPkx3eDB5cWmi1yv1nknhh3jY616/K033LRlfeBuGVuatLXWZvOFZgt9icHMNrC3dTt2ogd3ap5/SYrmtenen3xJCTYxjzcSx3frDO/gFwPpWgdkfoeI9buobaw6lF50SkPvBjYbeGRGQkMMoYM7iQdcYCYwHq1q3b8ciRI44O1TPF74EZ/cA/zOq/HFTCmZo2f241fg1512pfKOu2fAHf/h806Wf1BvGxr0pqQnI61Sr7FZzIMtPgwz6QfBr+b3XJP8+Syrhk/V47v2Vedk/Wtvo7r9/eucDVL2VkMfTd3zibnMH8x3tRM7SYJwFOdDY5nd5vLKNLw6p8eE+nq5bP2xzHX2b/zqQ72rt0TonM7BxmrT/KxMX7OJeSwS3to3i6XzO3takUR1kvOnc7RdwWMsZMM8bEGGNiIiLcU5OmwkmMs0pHePvB3fNKd9BqeyfU6WLdFrlUjLlo3WHb11bSanidNVDOziQAEB5UqfCrGV9/uPVDa0T2d48UWZPJoRLjrKS+8zu44Z8c6jWB2Vvi+Wn7qXxXN8bw4rfb2XcmmYm3tytTSQCgWlAlxvVuxOJdZ67qwms11u8lOiqEQfbMcOZAvt5ejO5Wn2XP9Obh3o1YsO0k1725jFcX7sq/aGE54dZEICKhwLXAd+6Mw+NcOmclgfRkGPVN6S9Xvbzgpres3hJL/p9jYnSGnd/D3LFQtzvcXoLbYPao3gJu/H+wbxFs+NDx75+fY+th2nVw7hDcORt6PsFjfZoQHRXC3+ZtIyE5/apNvoqNY+6m4zx+fRN6NSmbJ1fWgEN/Xlmw60/1lD5bc4TjF1J5rn8L+wvLOViIvy9/7d+cpU/3ZlCbWkxbcZDebyzlo98OkZHlhmJ1peS0RCAis4A1QDMRiROR+0VknIiMy7PaLcAiY0zB1buUY6UnW/WDLhyFO790XHfHmq2tonSxH1k9JtzNGDh/BLbOsSqmvtfTmuAjqqN1sHTmCM7OD0KTvrDoxSJHX5fa5pnw8U3gVxke/PXyKHBfby/eHtGOpPQsnp+77U+Nm7tOXuSl77bTo3E1Hu9TdBdNd/H39eapvs3YGpfID1ut0bwX0zJ5Z+l+ejUJp2eTcDdHCJFhAUwY0Y4fHu1Jy8gQ/vnDTvq+vZyF206Wqm6Sq+nENJ4kKwNmjYSDy617480HOvb90xLhnU4QEgkP/OqyyomAVVb51FY4ug6OrbXOkpNOWsv8gqxBO/V6QJeHCh3z4DDJZ2BKNwiuafUcsWMWsGLJzoLFf7dGCze4FoZ/nG+54g9XHuTf83fxxm1tGB5Th6S0TG5+5zdS0rOY/3gvp80P7SjZOYZBk1eRlJbJr09dy6Rf9/Hu0gP8+FhPoqPcX6wwL2MMy/bG8+qCXew9nUzHelV4YWALp9abKg63jCNwFk0ExWAMpF2wBr1cSoB1U2HHPBgyxZqUxBm2fgVzH7Bqp8SMcc4+wLoNdWyDddA/us66Csmy1eYPrQt1u1jtFnW6QI1Wrk1Kufb+bBUU6/Yo9PuP49439QJ8PcaaarLzQ1ZV2AK6/ObkWH3td5y4yMLxvXj9p90s2HaSWQ92pUtD987VbK/c8REP9mrAZ2uP0LdlTSbd4dyJWkojKzuHrzfG8dYve4lPSuem1rX4a/9mLqvbVBBNBBVFTrZ1AExJsA7sl3+ezf/5pbOQc8Uw9xv/BT3GOy9GY+CTwda4gsc2WuUoHCEzDXZ9D4dXWWf78bZbLuINtdr8cdCv29W6Iikr5j8NGz6wGuQbFTlesmgJ+2HW7Vbdm5vehI73FrlJ3PlL9J+4ksqVvDl9MZ2/9m/Gw70dW47B2e6ZsZ7le+Px9RZ+fbI3dau5pzhbcaSkZ/HByoNMW3GQzOwcRnWtR49G4Xh7CV5egrcIXl7gLfKn17y9BC/bT28vLj/2EiEkwLfE8ztrIihPsjKsiSnOHYSzB6yf52w/LxwDk53/dv6hEBhuHXgDw61BTlc+D60LES7oK35mN0ztAW1vt7qUlkZmGmz6FFZNsG71VAq1arTU6WKd9Ud1tO6Pl1WZqdZkI2mJVpfSyiU4C8/JtrY/uhbmjbPO/kd+DvW62/0WX8Ue45mvt3Jdswim39PJbY2sJbX71EUGTVrF3d3q8ffBrdwdTrGcuZjG24v3MnvDMYozh1B+xl1b8vmdNRGUNQUd7M8egMRjYPL0OqgUAlUbWqMVq9SHoBoQWC3PAT7ceu5dRgq45Vr0klW6d8wi64BdXFnpVgJYaZv/oG536P0s1L8m3xHAZdrJrdb4gsY3Qv9XrFs7aResq7vLj23Pcx9fXp4I6XkGLtWIhjtmWXVrisEYw4p9CXSsV8Xp5bGd5fiFVGqF+Je7JJbrZGIqCUkZZBtDdo4hJ/dnjiHbGLJyH19eBtnmj9eyjaFZjeASz7egicCdks/8UXPkxBZrZqurDvahUK2hdcCv2uiPA3/VhtZBviyMxC2u9GR4tzMEVLXq79hbtiI3Aax6Gy4etyZQ6f28NRlMefwccq1+BxZdPaPZZd5+1sC+gDBrpHbuY3/b84AwK/E3H1i2r4BUmVVYIiifpwZlVWaa1XMlLhbiNlgH/wtHrWVePlYf89ox0GbkHwf6qo2s3h7l+SCXn0pBViPmV/dYc+R2eajw9bPSYfNn1hXAxeNQp6s1UUeDayvGZ9P1YQitbc0ZfPkAH/bHQd83oGL8nqpc0kRQUn+qMrjBOvif2mYV+QIIqW0d9DuPhagYazYrN80+5DYth1gNpEv+DS2HQnA+Bc2y0q0SFSsnwMU4697/kHehYe+KdWD08oJWQ90dhVL50kRgr5wca+rGY+utM/24WEi1lVPwrWxV4uz2iHXwj4qBkDJSZtmdRGDgmzClK/zyEgyb9seyrAzY8jmseMtKALU7w5DJVumHipQAlCoHNBHYIz3ZKtO8+0dAIKK5da82KgZqd7Keu3CCiXKlWiPo/jisfBM6jLYO+Ftmwsq3rLaS2p3g5knWlYMmAKXcQo9eRUmMs/ptn95h1ZDpeC/4h7g7qvKl11NWqYdvH7ZuqSUetZLo4InQqI8mAKXcTBNBYeI2wpd3WOV975wDTW50d0Tlk18gDHzDKm8R1dEaddxYE4BSZYUmgoLklisOqgGjv7N6/KiSa9YfntprlbvWBKBUmaKJ4Eo5ObD8NVj+ujWIaeTnJRsNqq6WX68hpZTbaSLIK+MSfPewVZit3SgYNMHxVSOVUqqM0USQ6+JJqz3gxBarUbj7Y3oLQynlETQRgHXwn3W7NcXgHbOg2QB3R6SUUi6jiWDndzD3Iat425ifoWa0uyNSSimX8txEYIw1yGnJv61BTrfPLN0E7kopVU55ZiLITIPvH4Ntc6D1CLh5snMmMldKqXLA8xJB8hn48k6rUNz1L1mjXrVRWCnlwTwrEZzaDl+MtIrFjfgMWt7s7oiUUsrtPCcR7P8VZt9tTel430KIbOfuiJRSqkzwnERQpb41sfnQKRBc093RKKVUmeG0yV9FZIaInBGR7YWs01tEtojIDhFZ7qxYAKsc8t1zNQkopdQVnDkL+MdA/4IWikgYMAW42RjTChjuxFiUUkoVwGmJwBizAjhXyCp3AnONMUdt659xVixKKaUK5swrgqI0BaqIyDIR2SgiowtaUUTGikisiMTGx8e7MESllKr43JkIfICOwE1AP+AlEWma34rGmGnGmBhjTExERIQrY1RKqQrPnb2G4oCzxpgUIEVEVgBtgb1ujEkppTyOO68IvgN6ioiPiAQCXYBdboxHKaU8ktOuCERkFtAbCBeROODvgC+AMWaqMWaXiPwEbAVygA+NMQV2NVVKKeUcTksExpg77FjnDeANZ8WglFKqaGKMcXcMxSIi8cCREm4eDiQ4MBxHK+vxQdmPUeMrHY2vdMpyfPWMMfn2til3iaA0RCTWGBPj7jgKUtbjg7Ifo8ZXOhpf6ZT1+ArizsZipZRSZYAmAqWU8nCelgimuTuAIpT1+KDsx6jxlY7GVzplPb58eVQbgVJKqat52hWBUkqpK2giUEopD1chE4GI9BeRPSKyX0Sey2d5JRGZbVu+TkTquzC2OiKyVER22ibkGZ/POr1FJNE2ac8WEXnZVfHZ9n9YRLbZ9h2bz3IRkUm2z2+riHRwYWzN8nwuW0Tkoog8ccU6Lv/88puISUSqisgvIrLP9rNKAdveY1tnn4jc48L43hCR3bb/w3m2OULy27bQ74MT4/uHiBzP8/84sIBtC/17d2J8s/PEdlhEthSwrdM/v1IzxlSof4A3cABoCPgBvwMtr1jnYWCq7fHtwGwXxlcL6GB7HIxVZO/K+HoDP7rxMzwMhBeyfCCwEBCgK7DOjf/Xp7AGyrj18wOuAToA2/O89l/gOdvj54DX89muKnDQ9rOK7XEVF8XXF/CxPX49v/js+T44Mb5/AE/b8R0o9O/dWfFdsfwt4GV3fX6l/VcRrwg6A/uNMQeNMRnAl8CQK9YZAnxie/w10EdExBXBGWNOGmM22R4nYRXai3LFvh1oCPCpsawFwkSklhvi6AMcMMaUdKS5w5j8J2LK+z37BBiaz6b9gF+MMeeMMeeBXyhkZj9HxmeMWWSMybI9XQvUdvR+7VXA52cPe/7eS62w+GzHjhHALEfv11UqYiKIAo7leR7H1Qfay+vY/hASgWouiS4P2y2p9sC6fBZ3E5HfRWShiLRybWQYYJFtwqCx+Sy35zN2hdsp+I/PnZ9frhrGmJO2x6eAGvmsU1Y+yzFYV3n5Ker74EyP2m5dzSjg1lpZ+Px6AaeNMfsKWO7Oz88uFTERlAsiEgR8AzxhjLl4xeJNWLc72gKTgW9dHF5PY0wHYADwiIhc4+L9F0lE/ICbga/yWezuz+8qxrpHUCb7aovI34AsYGYBq7jr+/Ae0AhoB5zEuv1SFt1B4VcDZf7vqSImguNAnTzPa9tey3cdEfEBQoGzLonO2qcvVhKYaYyZe+VyY8xFY0yy7fECwFdEwl0VnzHmuO3nGWAe1uV3XvZ8xs42ANhkjDl95QJ3f355nM69ZWb7md+83G79LEXkXmAQcJctWV3Fju+DUxhjThtjso0xOcAHBezX3Z+fDzAMmF3QOu76/IqjIiaCDUATEWlgO2u8Hfj+inW+B3J7Z9wGLCnoj8DRbPcTpwO7jDETClinZm6bhYh0xvp/ckmiEpHKIhKc+xirQfHKeSK+B0bbeg91BRLz3AJxlQLPwtz5+V0h7/fsHqzJmK70M9BXRKrYbn30tb3mdCLSH/grcLMx5lIB69jzfXBWfHnbnW4pYL/2/L070w3AbmNMXH4L3fn5FYu7W6ud8Q+rV8terN4Ef7O99i+sLzyAP9Ythf3AeqChC2PriXWLYCuwxfZvIDAOGGdb51FgB1YPiLVAdxfG19C2399tMeR+fnnjE+Bd2+e7DYhx8f9vZawDe2ie19z6+WElpZNAJtZ96vux2p1+BfYBi4GqtnVjsCZiyt12jO27uB+4z4Xx7ce6v577PcztSRcJLCjs++Ci+D6zfb+2Yh3ca10Zn+35VX/vrojP9vrHud+7POu6/PMr7T8tMaGUUh6uIt4aUkopVQyaCJRSysNpIlBKKQ+niUAppTycJgKllPJwmgiUciFbZdQf3R2HUnlpIlBKKQ+niUCpfIjIKBFZb6sh/76IeItIsoi8LdY8Er+KSIRt3XYisjZPXf8qttcbi8hiW/G7TSLSyPb2QSLytW0ugJmuqnyrVEE0ESh1BRFpAYwEehhj2gHZwF1YI5pjjTGtgOXA322bfAo8a4xpgzUSNvf1mcC7xip+1x1rZCpYFWefAFpijTzt4eRfSalC+bg7AKXKoD5AR2CD7WQ9AKtgXA5/FBf7HJgrIqFAmDFmue31T4CvbPVloowx8wCMMWkAtvdbb2y1aWyzWtUHVjn9t1KqAJoIlLqaAJ8YY57/04siL12xXknrs6TneZyN/h0qN9NbQ0pd7VfgNhGpDpfnHq6H9fdym22dO4FVxphE4LyI9LK9fjew3Fizz8WJyFDbe1QSkUBX/hJK2UvPRJS6gjFmp4i8iDWrlBdWxclHgBSgs23ZGax2BLBKTE+1HegPAvfZXr8beF9E/mV7j+Eu/DWUsptWH1XKTiKSbIwJcnccSjma3hpSSikPp1cESinl4fSKQCmlPJwmAqWU8nCaCJRSysNpIlBKKQ+niUAppTzc/wcZgDh4n+k5NQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the result of Model 1 :\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(model3History.history['categorical_accuracy'])\n",
    "plt.plot(model3History.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model3History.history['loss'])\n",
    "plt.plot(model3History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth Base Model : Conv 3D Model with 30 epochs, 50 batch size Without dropouts in Conv layer and with batch normalization Input image size 100X100 , adam optimiser with learning rate 0.0002 without decay, 20 images as input out of 30. Dropout changed to 0.25. Image Scale Ratio: 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 20, 100, 100, 8)   656       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 20, 100, 100, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 20, 100, 100, 8)  32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 10, 50, 50, 8)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 10, 50, 50, 16)    3472      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 10, 50, 50, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 10, 50, 50, 16)   64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 5, 25, 25, 16)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 5, 25, 25, 32)     13856     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 5, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 5, 25, 25, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 2, 12, 12, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 2, 12, 12, 64)     55360     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 2, 12, 12, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 2, 12, 12, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 6, 6, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                147520    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 226,341\n",
      "Trainable params: 225,845\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "Total Params in model: 226341\n"
     ]
    }
   ],
   "source": [
    "modelConv3D_4 = Sequential()\n",
    "modelConv3D_4.add(Conv3D(8, (3,3,3), padding=\"same\", input_shape=(20, 100, 100, 3)))\n",
    "modelConv3D_4.add(Activation(\"relu\"))\n",
    "modelConv3D_4.add(BatchNormalization())\n",
    "modelConv3D_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_4.add(Conv3D(16, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_4.add(Activation(\"relu\"))\n",
    "modelConv3D_4.add(BatchNormalization())\n",
    "modelConv3D_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_4.add(Conv3D(32, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_4.add(Activation(\"relu\"))\n",
    "modelConv3D_4.add(BatchNormalization())\n",
    "modelConv3D_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_4.add(Conv3D(64, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_4.add(Activation(\"relu\"))\n",
    "modelConv3D_4.add(BatchNormalization())\n",
    "modelConv3D_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_4.add(Flatten())\n",
    "modelConv3D_4.add(Dense(64, activation='relu'))\n",
    "modelConv3D_4.add(BatchNormalization())\n",
    "modelConv3D_4.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_4.add(Dense(64, activation='relu'))\n",
    "modelConv3D_4.add(BatchNormalization())\n",
    "modelConv3D_4.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_4.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = tf.optimizers.Adam(lr=0.0002)\n",
    "\n",
    "# compile model :\n",
    "modelConv3D_4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary :\n",
    "print (modelConv3D_4.summary())\n",
    "print(\"Total Params in model:\", modelConv3D_4.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 50\n",
    "num_epochs = 30\n",
    "train_generator_updated = generator_updated(train_path, train_doc, batch_size)\n",
    "val_generator_updated = generator_updated(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose =1 )\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4History = modelConv3D_4.fit_generator(train_generator_updated, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator_updated, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t\t\t\n",
    "# Plotting the result of Model 1 :\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(model4History.history['categorical_accuracy'])\n",
    "plt.plot(model4History.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model4History.history['loss'])\n",
    "plt.plot(model4History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 5 : Conv 3D model\n",
    "batch size 30, increased image inputs to 30 , image size recuced to 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_updated_reduced_size(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0, 30)] \n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,30,84,84,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.03)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(84,84),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(84,84),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        data_remaining = len(t)%batch_size\n",
    "        if(data_remaining != 0):\n",
    "            batch_data = np.zeros((batch_size,30,84,84,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((data_remaining,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(data_remaining): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.03)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(84,84),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(84,84),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "num_epochs = 20\n",
    "train_generator_updated = generator_updated_reduced_size(train_path, train_doc, batch_size)\n",
    "val_generator_updated = generator_updated_reduced_size(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose =1 )\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_16 (Conv3D)          (None, 30, 84, 84, 8)     656       \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 30, 84, 84, 8)     0         \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 30, 84, 84, 8)    32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_16 (MaxPoolin  (None, 15, 42, 42, 8)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 15, 42, 42, 16)    3472      \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 15, 42, 42, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 15, 42, 42, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_17 (MaxPoolin  (None, 7, 21, 21, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 7, 21, 21, 32)     13856     \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 7, 21, 21, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 7, 21, 21, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 3, 10, 10, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 3, 10, 10, 64)     55360     \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 3, 10, 10, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 3, 10, 10, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPoolin  (None, 1, 5, 5, 64)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                102464    \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,285\n",
      "Trainable params: 180,789\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "Total Params in model: 181285\n"
     ]
    }
   ],
   "source": [
    "modelConv3D_5 = Sequential()\n",
    "modelConv3D_5.add(Conv3D(8, (3,3,3), padding=\"same\", input_shape=(30, 84, 84, 3)))\n",
    "modelConv3D_5.add(Activation(\"relu\"))\n",
    "modelConv3D_5.add(BatchNormalization())\n",
    "modelConv3D_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_5.add(Conv3D(16, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_5.add(Activation(\"relu\"))\n",
    "modelConv3D_5.add(BatchNormalization())\n",
    "modelConv3D_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_5.add(Conv3D(32, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_5.add(Activation(\"relu\"))\n",
    "modelConv3D_5.add(BatchNormalization())\n",
    "modelConv3D_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_5.add(Conv3D(64, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_5.add(Activation(\"relu\"))\n",
    "modelConv3D_5.add(BatchNormalization())\n",
    "modelConv3D_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_5.add(Flatten())\n",
    "modelConv3D_5.add(Dense(64, activation='relu'))\n",
    "modelConv3D_5.add(BatchNormalization())\n",
    "modelConv3D_5.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_5.add(Dense(64, activation='relu'))\n",
    "modelConv3D_5.add(BatchNormalization())\n",
    "modelConv3D_5.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_5.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = tf.optimizers.Adam(lr=0.0002)\n",
    "\n",
    "# compile model :\n",
    "modelConv3D_5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary :\n",
    "print (modelConv3D_5.summary())\n",
    "print(\"Total Params in model:\", modelConv3D_5.count_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267/3481440363.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model5History = modelConv3D_5.fit_generator(train_generator_updated, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/tmp/ipykernel_267/1639779493.py:15: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/23 [==========================>...] - ETA: 6s - loss: 1.8566 - categorical_accuracy: 0.3032 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267/1639779493.py:42: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
      "2024-11-05 15:05:17.366323: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must be broadcastable: logits_size=[30,5] labels_size=[3,5]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits\n (defined at /usr/local/lib/python3.8/dist-packages/keras/backend.py:5009)\n]] [Op:__inference_train_function_15907]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node categorical_crossentropy/softmax_cross_entropy_with_logits:\nIn[0] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape:\t\nIn[1] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_267/3481440363.py\", line 1, in <module>\n>>>     model5History = modelConv3D_5.fit_generator(train_generator_updated, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2016, in fit_generator\n>>>     return self.fit(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1664, in categorical_crossentropy\n>>>     return backend.categorical_crossentropy(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5009, in categorical_crossentropy\n>>>     return tf.nn.softmax_cross_entropy_with_logits(\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_267/3481440363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model5History = modelConv3D_5.fit_generator(train_generator_updated, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator_updated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2016\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[30,5] labels_size=[3,5]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits\n (defined at /usr/local/lib/python3.8/dist-packages/keras/backend.py:5009)\n]] [Op:__inference_train_function_15907]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node categorical_crossentropy/softmax_cross_entropy_with_logits:\nIn[0] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape:\t\nIn[1] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_267/3481440363.py\", line 1, in <module>\n>>>     model5History = modelConv3D_5.fit_generator(train_generator_updated, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2016, in fit_generator\n>>>     return self.fit(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1664, in categorical_crossentropy\n>>>     return backend.categorical_crossentropy(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5009, in categorical_crossentropy\n>>>     return tf.nn.softmax_cross_entropy_with_logits(\n>>> "
     ]
    }
   ],
   "source": [
    "model5History = modelConv3D_5.fit_generator(train_generator_updated, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator_updated, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result of Model 1 :\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(model5History.history['categorical_accuracy'])\n",
    "plt.plot(model5History.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model5History.history['loss'])\n",
    "plt.plot(model5History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 6 : CNN - LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_50 (TimeDi  (None, 20, 100, 100, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_51 (TimeDi  (None, 20, 100, 100, 16)  64       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_52 (TimeDi  (None, 20, 50, 50, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_53 (TimeDi  (None, 20, 50, 50, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_54 (TimeDi  (None, 20, 50, 50, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_55 (TimeDi  (None, 20, 25, 25, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_56 (TimeDi  (None, 20, 25, 25, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_57 (TimeDi  (None, 20, 25, 25, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_58 (TimeDi  (None, 20, 12, 12, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_59 (TimeDi  (None, 20, 12, 12, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_60 (TimeDi  (None, 20, 12, 12, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_61 (TimeDi  (None, 20, 6, 6, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_62 (TimeDi  (None, 20, 6, 6, 256)    295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_63 (TimeDi  (None, 20, 6, 6, 256)    1024      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_64 (TimeDi  (None, 20, 3, 3, 256)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_65 (TimeDi  (None, 20, 2304)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               1245696   \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,657,445\n",
      "Trainable params: 1,656,453\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape=(20,100,100,3)\n",
    "CNNlstm_1 = Sequential()\n",
    "\n",
    "CNNlstm_1.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                          input_shape=input_shape))\n",
    "CNNlstm_1.add(TimeDistributed(BatchNormalization()))\n",
    "CNNlstm_1.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "CNNlstm_1.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "CNNlstm_1.add(TimeDistributed(BatchNormalization()))\n",
    "CNNlstm_1.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "CNNlstm_1.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "CNNlstm_1.add(TimeDistributed(BatchNormalization()))\n",
    "CNNlstm_1.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "CNNlstm_1.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "CNNlstm_1.add(TimeDistributed(BatchNormalization()))\n",
    "CNNlstm_1.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "CNNlstm_1.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
    "CNNlstm_1.add(TimeDistributed(BatchNormalization()))\n",
    "CNNlstm_1.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "CNNlstm_1.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "CNNlstm_1.add(LSTM(128))\n",
    "CNNlstm_1.add(Dropout(0.25))\n",
    "\n",
    "CNNlstm_1.add(Dense(128,activation='relu'))\n",
    "CNNlstm_1.add(Dropout(0.25))\n",
    "\n",
    "CNNlstm_1.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "#write your optimizer\n",
    "optimiser = tf.optimizers.Adam()\n",
    "#optimiser = \n",
    "#sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "\n",
    "# compile model :\n",
    "CNNlstm_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (CNNlstm_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorLSTM(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = arr\n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,20,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.10)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        data_remaining = len(t)%batch_size\n",
    "        if(data_remaining != 0):\n",
    "            batch_data = np.zeros((data_remaining,20,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((data_remaining,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(data_remaining): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.10)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "num_epochs = 20\n",
    "train_generator = generatorLSTM(train_path, train_doc, batch_size)\n",
    "val_generator = generatorLSTM(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4)\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267/2841256302.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = CNNlstm_1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/tmp/ipykernel_267/3530338047.py:15: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 30\n",
      "Epoch 1/20\n",
      " 2/23 [=>............................] - ETA: 33s - loss: 1.6826 - categorical_accuracy: 0.2667 "
     ]
    }
   ],
   "source": [
    "history = CNNlstm_1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
