{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.1.0\n",
      "  Using cached scipy-1.1.0-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from scipy==1.1.0) (1.19.4)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-image 0.19.2 requires scipy>=1.4.1, but you have scipy 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy==1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "#batch_size = #experiment with the batch size\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to crop image :\n",
    "\n",
    "def crop_img(img, scale=1.0):\n",
    "    center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n",
    "    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n",
    "    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "    img_cropped = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n",
    "    return img_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  7  8  9 11 15 16 19 20 21 22 22 26 26 26 26 27 28]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(20)\n",
    "random_numbers = np.random.randint(0, 31, size=20)  # 31 is exclusive\n",
    "arr = np.sort(random_numbers)\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = arr\n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,20,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.10)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        data_remaining = len(t)%batch_size\n",
    "        if(data_remaining != 0):\n",
    "            batch_data = np.zeros((data_remaining,20,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((data_remaining,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(data_remaining): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = crop_img(image, 0.10)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(100,100),interpolation = cv2.INTER_AREA).astype(np.float32)                    \n",
    "                       \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalize and feed the image. \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 5\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/datasets/Project_data/train'\n",
    "val_path = '/datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 5\n",
    "# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Base Model : Conv 3D Model with 5 epochs, 25 batch size\n",
    "Without dropouts in Conv layer and with batch normalization\n",
    "Input image size 100X100 , adam optimiser with learning rate 0.0002 with decay, 20 images as input out of 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_42 (Conv3D)          (None, 20, 100, 100, 8)   656       \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 20, 100, 100, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 20, 100, 100, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_41 (MaxPoolin  (None, 10, 50, 50, 8)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_43 (Conv3D)          (None, 10, 50, 50, 16)    3472      \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 10, 50, 50, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_60 (Bat  (None, 10, 50, 50, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_42 (MaxPoolin  (None, 5, 25, 25, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_44 (Conv3D)          (None, 5, 25, 25, 32)     13856     \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 5, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_61 (Bat  (None, 5, 25, 25, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_43 (MaxPoolin  (None, 2, 12, 12, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_45 (Conv3D)          (None, 2, 12, 12, 64)     55360     \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 2, 12, 12, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_62 (Bat  (None, 2, 12, 12, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_44 (MaxPoolin  (None, 1, 6, 6, 64)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                147520    \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 226,341\n",
      "Trainable params: 225,845\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "Total Params in model: 226341\n"
     ]
    }
   ],
   "source": [
    "modelConv3D_1 = Sequential()\n",
    "modelConv3D_1.add(Conv3D(8, (3,3,3), padding=\"same\", input_shape=(20, 100, 100, 3)))\n",
    "modelConv3D_1.add(Activation(\"relu\"))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_1.add(Conv3D(16, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_1.add(Activation(\"relu\"))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_1.add(Conv3D(32, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_1.add(Activation(\"relu\"))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_1.add(Conv3D(64, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_1.add(Activation(\"relu\"))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_1.add(Flatten())\n",
    "modelConv3D_1.add(Dense(64, activation='relu'))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_1.add(Dense(64, activation='relu'))\n",
    "modelConv3D_1.add(BatchNormalization())\n",
    "modelConv3D_1.add(Dropout(0.25))\n",
    "\n",
    "modelConv3D_1.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = tf.optimizers.Adam(lr=0.0002, decay=1e-6)\n",
    "\n",
    "# compile model :\n",
    "modelConv3D_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary :\n",
    "print (modelConv3D_1.summary())\n",
    "print(\"Total Params in model:\", modelConv3D_1.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_42 (Conv3D)          (None, 20, 100, 100, 8)   656       \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 20, 100, 100, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 20, 100, 100, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_41 (MaxPoolin  (None, 10, 50, 50, 8)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_43 (Conv3D)          (None, 10, 50, 50, 16)    3472      \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 10, 50, 50, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_60 (Bat  (None, 10, 50, 50, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_42 (MaxPoolin  (None, 5, 25, 25, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_44 (Conv3D)          (None, 5, 25, 25, 32)     13856     \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 5, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_61 (Bat  (None, 5, 25, 25, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_43 (MaxPoolin  (None, 2, 12, 12, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_45 (Conv3D)          (None, 2, 12, 12, 64)     55360     \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 2, 12, 12, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_62 (Bat  (None, 2, 12, 12, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_44 (MaxPoolin  (None, 1, 6, 6, 64)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                147520    \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 226,341\n",
      "Trainable params: 225,845\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#optimiser = #write your optimizer\n",
    "modelConv3D_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (modelConv3D_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203/2262592253.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model1History = modelConv3D_1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/tmp/ipykernel_203/164759091.py:15: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25/27 [==========================>...] - ETA: 2s - loss: 1.2413 - categorical_accuracy: 0.5200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203/164759091.py:42: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - ETA: 0s - loss: 1.2466 - categorical_accuracy: 0.5173\n",
      "Epoch 00001: saving model to model_init_2024-11-0510_55_23.161267/model-00001-1.24661-0.51735-2.27099-0.14000.h5\n",
      "27/27 [==============================] - 36s 1s/step - loss: 1.2466 - categorical_accuracy: 0.5173 - val_loss: 2.2710 - val_categorical_accuracy: 0.1400 - lr: 4.0000e-05\n",
      "Epoch 2/5\n",
      "27/27 [==============================] - ETA: 0s - loss: 1.2323 - categorical_accuracy: 0.5038\n",
      "Epoch 00002: saving model to model_init_2024-11-0510_55_23.161267/model-00002-1.23227-0.50377-2.19938-0.21000.h5\n",
      "27/27 [==============================] - 36s 1s/step - loss: 1.2323 - categorical_accuracy: 0.5038 - val_loss: 2.1994 - val_categorical_accuracy: 0.2100 - lr: 4.0000e-05\n",
      "Epoch 3/5\n",
      "27/27 [==============================] - ETA: 0s - loss: 1.1517 - categorical_accuracy: 0.5535\n",
      "Epoch 00003: saving model to model_init_2024-11-0510_55_23.161267/model-00003-1.15171-0.55354-2.28031-0.24000.h5\n",
      "27/27 [==============================] - 36s 1s/step - loss: 1.1517 - categorical_accuracy: 0.5535 - val_loss: 2.2803 - val_categorical_accuracy: 0.2400 - lr: 4.0000e-05\n",
      "Epoch 4/5\n",
      "27/27 [==============================] - ETA: 0s - loss: 1.1982 - categorical_accuracy: 0.5294\n",
      "Epoch 00004: saving model to model_init_2024-11-0510_55_23.161267/model-00004-1.19825-0.52941-2.32908-0.21000.h5\n",
      "27/27 [==============================] - 36s 1s/step - loss: 1.1982 - categorical_accuracy: 0.5294 - val_loss: 2.3291 - val_categorical_accuracy: 0.2100 - lr: 4.0000e-05\n",
      "Epoch 5/5\n",
      "27/27 [==============================] - ETA: 0s - loss: 1.1120 - categorical_accuracy: 0.5913\n",
      "Epoch 00005: saving model to model_init_2024-11-0510_55_23.161267/model-00005-1.11199-0.59125-2.31359-0.18000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "27/27 [==============================] - 37s 1s/step - loss: 1.1120 - categorical_accuracy: 0.5913 - val_loss: 2.3136 - val_categorical_accuracy: 0.1800 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "model1History = modelConv3D_1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the result of Model 1 :\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(model1History.history['categorical_accuracy'])\n",
    "plt.plot(model1History.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model1History.history['loss'])\n",
    "plt.plot(model1History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Base Model : Conv 3D Model with 20 epochs, 50 batch size Without dropouts in Conv layer and with batch normalization Input image size 100X100 , adam optimiser with learning rate 0.0002 without decay, 20 images as input out of 30. Dropout changed to 0.5\n",
    "\n",
    "Now that you have written the model, the next step is to compile the model. When you print the summary of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_46 (Conv3D)          (None, 20, 100, 100, 8)   656       \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 20, 100, 100, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization_65 (Bat  (None, 20, 100, 100, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_45 (MaxPoolin  (None, 10, 50, 50, 8)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_47 (Conv3D)          (None, 10, 50, 50, 16)    3472      \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 10, 50, 50, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_66 (Bat  (None, 10, 50, 50, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_46 (MaxPoolin  (None, 5, 25, 25, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_48 (Conv3D)          (None, 5, 25, 25, 32)     13856     \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 5, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_67 (Bat  (None, 5, 25, 25, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_47 (MaxPoolin  (None, 2, 12, 12, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_49 (Conv3D)          (None, 2, 12, 12, 64)     55360     \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 2, 12, 12, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 2, 12, 12, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_48 (MaxPoolin  (None, 1, 6, 6, 64)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                147520    \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_70 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 226,341\n",
      "Trainable params: 225,845\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "Total Params in model: 226341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "modelConv3D_2 = Sequential()\n",
    "modelConv3D_2.add(Conv3D(8, (3,3,3), padding=\"same\", input_shape=(20, 100, 100, 3)))\n",
    "modelConv3D_2.add(Activation(\"relu\"))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_2.add(Conv3D(16, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_2.add(Activation(\"relu\"))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_2.add(Conv3D(32, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_2.add(Activation(\"relu\"))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "modelConv3D_2.add(Conv3D(64, (3,3,3), padding=\"same\"))\n",
    "modelConv3D_2.add(Activation(\"relu\"))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "modelConv3D_2.add(Flatten())\n",
    "modelConv3D_2.add(Dense(64, activation='relu'))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(Dropout(0.5))\n",
    "\n",
    "modelConv3D_2.add(Dense(64, activation='relu'))\n",
    "modelConv3D_2.add(BatchNormalization())\n",
    "modelConv3D_2.add(Dropout(0.5))\n",
    "\n",
    "modelConv3D_2.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = tf.optimizers.Adam(lr=0.0002)\n",
    "\n",
    "# compile model :\n",
    "modelConv3D_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary :\n",
    "print (modelConv3D_2.summary())\n",
    "print(\"Total Params in model:\", modelConv3D_2.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 20\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose =1 )\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203/254883775.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model2History = modelConv3D_2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/tmp/ipykernel_203/164759091.py:15: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/14 [======================>.......] - ETA: 6s - loss: 2.5370 - categorical_accuracy: 0.2273"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203/164759091.py:42: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 2.4700 - categorical_accuracy: 0.2383Source path =  /datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: saving model to model_init_2024-11-0510_55_23.161267/model-00001-2.46996-0.23831-1.62256-0.21000.h5\n",
      "14/14 [==============================] - 37s 3s/step - loss: 2.4700 - categorical_accuracy: 0.2383 - val_loss: 1.6226 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.3580 - categorical_accuracy: 0.2730\n",
      "Epoch 00002: saving model to model_init_2024-11-0510_55_23.161267/model-00002-2.35799-0.27300-1.69005-0.15000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 2.3580 - categorical_accuracy: 0.2730 - val_loss: 1.6900 - val_categorical_accuracy: 0.1500 - lr: 2.0000e-04\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0721 - categorical_accuracy: 0.3484\n",
      "Epoch 00003: saving model to model_init_2024-11-0510_55_23.161267/model-00003-2.07209-0.34842-1.72013-0.21000.h5\n",
      "14/14 [==============================] - 37s 3s/step - loss: 2.0721 - categorical_accuracy: 0.3484 - val_loss: 1.7201 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0337 - categorical_accuracy: 0.3424\n",
      "Epoch 00004: saving model to model_init_2024-11-0510_55_23.161267/model-00004-2.03372-0.34238-1.75112-0.21000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 2.0337 - categorical_accuracy: 0.3424 - val_loss: 1.7511 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0596 - categorical_accuracy: 0.3047\n",
      "Epoch 00005: saving model to model_init_2024-11-0510_55_23.161267/model-00005-2.05964-0.30468-1.81990-0.21000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "14/14 [==============================] - 37s 3s/step - loss: 2.0596 - categorical_accuracy: 0.3047 - val_loss: 1.8199 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7999 - categorical_accuracy: 0.3771\n",
      "Epoch 00006: saving model to model_init_2024-11-0510_55_23.161267/model-00006-1.79995-0.37707-1.90765-0.18000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.7999 - categorical_accuracy: 0.3771 - val_loss: 1.9076 - val_categorical_accuracy: 0.1800 - lr: 4.0000e-05\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0619 - categorical_accuracy: 0.3107\n",
      "Epoch 00007: saving model to model_init_2024-11-0510_55_23.161267/model-00007-2.06194-0.31071-1.94891-0.21000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 2.0619 - categorical_accuracy: 0.3107 - val_loss: 1.9489 - val_categorical_accuracy: 0.2100 - lr: 4.0000e-05\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8933 - categorical_accuracy: 0.3575\n",
      "Epoch 00008: saving model to model_init_2024-11-0510_55_23.161267/model-00008-1.89329-0.35747-2.03082-0.20000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.8933 - categorical_accuracy: 0.3575 - val_loss: 2.0308 - val_categorical_accuracy: 0.2000 - lr: 4.0000e-05\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7740 - categorical_accuracy: 0.3725\n",
      "Epoch 00009: saving model to model_init_2024-11-0510_55_23.161267/model-00009-1.77401-0.37255-2.08184-0.21000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "14/14 [==============================] - 37s 3s/step - loss: 1.7740 - categorical_accuracy: 0.3725 - val_loss: 2.0818 - val_categorical_accuracy: 0.2100 - lr: 4.0000e-05\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7756 - categorical_accuracy: 0.3967\n",
      "Epoch 00010: saving model to model_init_2024-11-0510_55_23.161267/model-00010-1.77564-0.39668-2.22135-0.17000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.7756 - categorical_accuracy: 0.3967 - val_loss: 2.2214 - val_categorical_accuracy: 0.1700 - lr: 8.0000e-06\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8219 - categorical_accuracy: 0.3876\n",
      "Epoch 00011: saving model to model_init_2024-11-0510_55_23.161267/model-00011-1.82186-0.38763-2.20664-0.21000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.8219 - categorical_accuracy: 0.3876 - val_loss: 2.2066 - val_categorical_accuracy: 0.2100 - lr: 8.0000e-06\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8234 - categorical_accuracy: 0.3861\n",
      "Epoch 00012: saving model to model_init_2024-11-0510_55_23.161267/model-00012-1.82343-0.38612-2.28969-0.21000.h5\n",
      "14/14 [==============================] - 37s 3s/step - loss: 1.8234 - categorical_accuracy: 0.3861 - val_loss: 2.2897 - val_categorical_accuracy: 0.2100 - lr: 8.0000e-06\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7514 - categorical_accuracy: 0.4223\n",
      "Epoch 00013: saving model to model_init_2024-11-0510_55_23.161267/model-00013-1.75142-0.42232-2.31011-0.21000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5999999959603884e-06.\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.7514 - categorical_accuracy: 0.4223 - val_loss: 2.3101 - val_categorical_accuracy: 0.2100 - lr: 8.0000e-06\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7867 - categorical_accuracy: 0.3650\n",
      "Epoch 00014: saving model to model_init_2024-11-0510_55_23.161267/model-00014-1.78674-0.36501-2.26413-0.23000.h5\n",
      "14/14 [==============================] - 37s 3s/step - loss: 1.7867 - categorical_accuracy: 0.3650 - val_loss: 2.2641 - val_categorical_accuracy: 0.2300 - lr: 1.6000e-06\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8259 - categorical_accuracy: 0.3575\n",
      "Epoch 00015: saving model to model_init_2024-11-0510_55_23.161267/model-00015-1.82594-0.35747-2.36273-0.21000.h5\n",
      "14/14 [==============================] - 37s 3s/step - loss: 1.8259 - categorical_accuracy: 0.3575 - val_loss: 2.3627 - val_categorical_accuracy: 0.2100 - lr: 1.6000e-06\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7865 - categorical_accuracy: 0.3967\n",
      "Epoch 00016: saving model to model_init_2024-11-0510_55_23.161267/model-00016-1.78649-0.39668-2.31655-0.25000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.7865 - categorical_accuracy: 0.3967 - val_loss: 2.3166 - val_categorical_accuracy: 0.2500 - lr: 1.6000e-06\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8117 - categorical_accuracy: 0.3620\n",
      "Epoch 00017: saving model to model_init_2024-11-0510_55_23.161267/model-00017-1.81171-0.36199-2.38227-0.21000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000037395512e-07.\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.8117 - categorical_accuracy: 0.3620 - val_loss: 2.3823 - val_categorical_accuracy: 0.2100 - lr: 1.6000e-06\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8541 - categorical_accuracy: 0.3741\n",
      "Epoch 00018: saving model to model_init_2024-11-0510_55_23.161267/model-00018-1.85412-0.37406-2.47492-0.19000.h5\n",
      "14/14 [==============================] - 37s 3s/step - loss: 1.8541 - categorical_accuracy: 0.3741 - val_loss: 2.4749 - val_categorical_accuracy: 0.1900 - lr: 3.2000e-07\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8439 - categorical_accuracy: 0.3680\n",
      "Epoch 00019: saving model to model_init_2024-11-0510_55_23.161267/model-00019-1.84391-0.36802-2.38353-0.22000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.8439 - categorical_accuracy: 0.3680 - val_loss: 2.3835 - val_categorical_accuracy: 0.2200 - lr: 3.2000e-07\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8232 - categorical_accuracy: 0.3514\n",
      "Epoch 00020: saving model to model_init_2024-11-0510_55_23.161267/model-00020-1.82318-0.35143-2.40368-0.21000.h5\n",
      "14/14 [==============================] - 36s 3s/step - loss: 1.8232 - categorical_accuracy: 0.3514 - val_loss: 2.4037 - val_categorical_accuracy: 0.2100 - lr: 3.2000e-07\n"
     ]
    }
   ],
   "source": [
    "model2History = modelConv3D_2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
